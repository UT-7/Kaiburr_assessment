# -*- coding: utf-8 -*-
"""kaiburr_task_6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1avlwJxx7kcxijctJG9eJZ61SMQp3rKxK
"""

# Import the required libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
import seaborn as sns
from sklearn.metrics import mean_squared_error, r2_score

# Load the dataset from the CSV file
data = pd.read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv', header=None)

# Separate the features and target variable
X = data.iloc[:, :-1].values
y = data.iloc[:, -1].values

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Random Forest Regressor with default hyperparameters
rf_regressor = RandomForestRegressor()

# Train the model on the training data
rf_regressor.fit(X_train, y_train)

y_pred = rf_regressor.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error: ", mse)

print(data.dtypes)

fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(20, 20))
for i, ax in enumerate(axes.flat):
    if i < data.shape[1]:
        sns.histplot(data.iloc[:, i], ax=ax)
plt.show()

corr = data.corr()
sns.heatmap(corr, annot=True, cmap='coolwarm')
plt.show()

from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X, y)

new_data = np.array([0.1, 18.0, 2.0, 0, 0.5, 6.0, 70.0, 3.0, 20.0, 300.0, 15.3, 396.9, 4.98]).reshape(1, -1)
prediction = model.predict(new_data)

print(prediction)

new_sample = [[0.00632, 18.0, 2.31, 0, 0.538, 6.575, 65.2, 4.0900, 1, 296.0, 15.3, 396.90, 4.98]]
prediction = model.predict(new_sample)

print("Predicted median value of owner-occupied homes in $1000's:", prediction[0])